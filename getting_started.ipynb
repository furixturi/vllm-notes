{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM on Azure Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [Quickstart doc](https://docs.vllm.ai/en/stable/getting_started/quickstart.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spin up an H100 compute instance (e.g. `Standard_NC80adis_H100_v5` which has 2 H100 GPUs, each with 94GiB GPU memory) in Azure Machine Learning, open `Terminal` app and install vLLM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ conda create -n myenv python=3.10 -y\n",
    "$ conda activate myenv\n",
    "$ pip install vllm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an OpenAI-compatible LLM server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vLLM can be used as a server that implements OpemAI API protocol. Simply run it with a model available on Hugging Face, e.g. [Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) as follows:\n",
    "\n",
    "```bash\n",
    "$ vllm serve Qwen/Qwen2.5-1.5B-Instruct\n",
    "```\n",
    "\n",
    "By default, the server starts at `localhost` with port `8000`.\n",
    "\n",
    "vLLM supports many models from [HuggingFace Transformers](https://huggingface.co/models). A list of supported models can be found [here](https://docs.vllm.ai/en/latest/models/supported_models.html#supported-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the server running, you can simply call the list models API with a cURL command like this:\n",
    "\n",
    "```bash\n",
    "$ curl http://localhost:8000/v1/models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with Python `requests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"object\":\"model\",\"created\":1733570012,\"owned_by\":\"vllm\",\"root\":\"Qwen/Qwen2.5-1.5B-Instruct\",\"parent\":null,\"max_model_len\":32768,\"permission\":[{\"id\":\"modelperm-43718e5e43ae4f749f8a59f8233dbe2b\",\"object\":\"model_permission\",\"created\":1733570012,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OpenAI client to call the Chat completion API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "completion = client.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    prompt=\"San Francisco is a\",\n",
    "    max_tokens=7,\n",
    "    temperature=0.7,\n",
    ")\n",
    "print(\"Completion result:\", completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Chat Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let vllm serve Phi-3.5 mini:\n",
    "\n",
    "```bash\n",
    "$ vllm serve microsoft/Phi-3.5-mini-instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With cURL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the model list API again\n",
    "\n",
    "```bash\n",
    "$ curl http://localhost:8000/v1/models\n",
    "```\n",
    "\n",
    "And we'll see it's `\"microsoft/Phi-3.5-mini-instruct\"`\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"object\":\"list\",\n",
    "   \"data\":[\n",
    "      {\n",
    "         \"id\":\"microsoft/Phi-3.5-mini-instruct\",\n",
    "         \"object\":\"model\",\n",
    "         \"created\":1733574665,\n",
    "         \"owned_by\":\"vllm\",\n",
    "         \"root\":\"microsoft/Phi-3.5-mini-instruct\",\n",
    "         \"parent\":null,\n",
    "         \"max_model_len\":131072,\n",
    "         \"permission\":[\n",
    "            {\n",
    "               \"id\":\"modelperm-bfdf3099c73443f1a7b222ca7b344a90\",\n",
    "               \"object\":\"model_permission\",\n",
    "               \"created\":1733574665,\n",
    "               \"allow_create_engine\":false,\n",
    "               \"allow_sampling\":true,\n",
    "               \"allow_logprobs\":true,\n",
    "               \"allow_search_indices\":false,\n",
    "               \"allow_view\":true,\n",
    "               \"allow_fine_tuning\":false,\n",
    "               \"organization\":\"*\",\n",
    "               \"group\":null,\n",
    "               \"is_blocking\":false\n",
    "            }\n",
    "         ]\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the chat completion API. A cURL request would look like this:\n",
    "\n",
    "```bash\n",
    "curl http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"microsoft/Phi-3.5-mini-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "        ]\n",
    "    }'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the response\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"id\":\"chatcmpl-3022a77606db41ac997645b339ea6a78\",\n",
    "   \"object\":\"chat.completion\",\n",
    "   \"created\":1733574718,\n",
    "   \"model\":\"microsoft/Phi-3.5-mini-instruct\",\n",
    "   \"choices\":[\n",
    "      {\n",
    "         \"index\":0,\n",
    "         \"message\":{\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\":\" The world series in Major League Baseball (MLB) for the 2020 season was won by the Los Angeles Dodgers. The Dodgers defeated the Tampa Bay Rays in seven games, with the series concluding on October 30, 2020. This was a significant year because it was shortened due to the COVID-19 pandemic, resulting in a regular season of just 60 games. The Dodgers' victory was a culmination of a strong regular season and postseason performance.\",\n",
    "            \"tool_calls\":[\n",
    "               \n",
    "            ]\n",
    "         },\n",
    "         \"logprobs\":null,\n",
    "         \"finish_reason\":\"stop\",\n",
    "         \"stop_reason\":32007\n",
    "      }\n",
    "   ],\n",
    "   \"usage\":{\n",
    "      \"prompt_tokens\":23,\n",
    "      \"total_tokens\":138,\n",
    "      \"completion_tokens\":115,\n",
    "      \"prompt_tokens_details\":null\n",
    "   },\n",
    "   \"prompt_logprobs\":null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With OpenAI Python client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use OpenAI Python client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response:  Sure, here's a tech-themed joke for you:\n",
      "\n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n",
      "\n",
      "This joke plays on the double meaning of \"light.\" In the context of a computer screen, light mode refers to the default, bright display. However, \"light\" is also commonly used to refer to bugs or glitches in programming. So, when programmers say they prefer \"dark mode,\" they're humorously suggesting that a darker screen might help them avoid attracting bugs in their code.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a geeky joke.\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Free up GPU memory\n",
    "\n",
    "If the first model is still in the memory, loading a second model and start a server could encounter a `CUDA out of memory` error.\n",
    "\n",
    "```bash\n",
    "ERROR 12-07 12:14:35 engine.py:366] CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 93.02 GiB of which 40.19 MiB is free. Process 24272 has 82.55 GiB memory in use. Process 72349 has 5.78 GiB memory in use. Including non-PyTorch memory, this process has 4.60 GiB memory in use. Of the allocated memory 3.98 GiB is allocated by PyTorch, and 26.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "```\n",
    "\n",
    "Kill the process that takes up the biggest GPU memory (whose PID is shown in the error message as 24272):\n",
    "\n",
    "```bash\n",
    "$ kill -9 24272\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `LLM` class for offline inference with vLLM engine. `SamplingParams` speifies the parameters for the sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-07 11:59:37,190\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(temperature = 0.8, top_p = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-07 12:06:10 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=facebook/opt-125m, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 12-07 12:06:11 model_runner.py:1072] Starting to load model facebook/opt-125m...\n",
      "INFO 12-07 12:06:11 weight_utils.py:243] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.83it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-07 12:06:11 model_runner.py:1077] Loading model weights took 0.2389 GB\n",
      "INFO 12-07 12:06:11 worker.py:232] Memory profiling results: total_gpu_memory=93.02GiB initial_memory_usage=83.77GiB peak_torch_memory=0.97GiB memory_usage_post_profile=83.77GiB non_torch_memory=83.26GiB kv_cache_size=4.14GiB gpu_memory_utilization=0.95\n",
      "INFO 12-07 12:06:11 gpu_executor.py:113] # GPU blocks: 7529, # CPU blocks: 7281\n",
      "INFO 12-07 12:06:11 gpu_executor.py:117] Maximum concurrency for 2048 tokens per request: 58.82x\n",
      "INFO 12-07 12:06:14 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-07 12:06:14 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-07 12:06:24 model_runner.py:1518] Graph capturing finished in 10 secs, took 0.44 GiB\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"facebook/opt-125m\", gpu_memory_utilization=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I ran into `No available memory for the cache blocks` error and added `gpu_memory_utilization=0.95`.)\n",
    "\n",
    "Now run a batch LLM generation with `llm.generate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 72.71it/s, est. speed input: 473.54 toks/s, output: 1165.49 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: ' Joel, my dad is my friend and we are in a relationship. I am'\n",
      "Prompt: 'The president of the United States is', Generated text: ' speaking out against the release of some State Department documents which show the Russians were involved'\n",
      "Prompt: 'The capital of France is', Generated text: ' known as the “Proud French capital”. What is this city'\n",
      "Prompt: 'The future of AI is', Generated text: ' literally in danger of being taken by any other company.\\nAgreed. '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
