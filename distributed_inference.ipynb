{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Inference and Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common distributed inference strategies according to your hardware resources:\n",
    "- Single GPU\n",
    "    - No distributed inference.\n",
    "- Single-node, multi-GPU\n",
    "    - tensor parallel: model is too large to fit in one GPU but fit in multiple GPUs within a single node.\n",
    "        - tensor parallel size = number of GPUs\n",
    "- Multi-node, multi-GPU\n",
    "    - tensor parallel + pipeline parallel\n",
    "        - tensor parallel size = number of GPUs per node\n",
    "        - pipeline parallel size = number of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference memory calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ M = \\frac{P * 4}{32/Q} * 1.2 (GB) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $M$: GPU memory\n",
    "- $P$: parameters (in Billions, 7B is 7)\n",
    "- $4$: 4 bytes for each parameter\n",
    "- $32$: 1 byte is 8 bits, so 4 bytes is 32 bits\n",
    "- $Q$: quantization bits (e.g., 16 bits, 8 bits, 4 bits)\n",
    "- $1.2$: 20% overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory for loading 70B model at 16 bit precision is\n",
    "\n",
    "$$ \\frac{70 * 4}{32/16} * 1.2 = 168GB $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
